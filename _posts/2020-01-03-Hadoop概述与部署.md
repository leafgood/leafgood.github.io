---
layout: post
title: '01Hadoop概述与部署'
date: 2020-01-03
author: leafgood
color: rgb(255,210,32)
tags: Hadoop入门与进阶
---
# Hadoop概述与部署
参考: http://hadoop.apache.org/docs/r3.1.3/index.html
## 1.Hadoop概述
- 什么是Hadoop？
Hadoop是由Apache基金会开发的分布式系统基础架构，用来解决海量数据的存储和分析计算问题。
- Hadoop的优势
	- 高可靠：Hadoop数据存储底层采用多副本
	- 高扩展：集群部署，可以轻松进行节点的扩展
	- 高效性：Hadoop并行工作，加快任务处理速度
	- 高容错：能够自动地将失败的任务重新分配
## 2.Hadoop组成
-  2.x版本和3.x版本组成如下：
	- MapReduce：计算
	- Yarn：资源调度
	- HDFS：数据存储
	- Common：辅助工具

备注：1.x版本中，没有Yarn，MapRedcuer承担计算和资源调度 
## 3.部署规划
- 三台虚拟机
| IP- 主机名  |  操作系统 | 配置  | 节点 
| --- | -----| ---- | -------|
| 192.168.122.10-Hadoop10 | CentOS 7.5 | 1核/4G内存/50G硬盘 |NameNode、DataNode、NodeManager |
| 192.168.122.11-Hadoop11 | CentOS 7.5 | 1核/4G内存/50G硬盘 |ResourceManager、DataNode、NodeManager|
| 192.168.122.12-Hadoop12 | CentOS 7.5 | 1核/4G内存/50G硬盘 |SecondaryNameNode、DataNode、NodeManager

## 4.集群部署
### 4.1 系统更新和ssh免密配置
-  更新升级
```
yum install  -y epel-release
yum update
```
- 配置ssh免密登录
```
[v2admin@hadoop10 ~]$ ssh-keygen -t rsa
//...连续回车即可生成私钥id_rsa和id_rsa.pub
// 我的用户是v2admin，后续操作都是以这个用户
[v2admin@hadoop10 ~]$ ssh-copy-id hadoop10
[v2admin@hadoop10 ~]$ ssh-copy-id hadoop11
[v2admin@hadoop10 ~]$ ssh-copy-id hadoop12
// hadoop11 hadoop12 执行同样操作
```
- 上传jdk和Hadoop包至三台虚拟机的/home/v2admin目录下
```
// 我自己的用操作系统是Ubuntu 18.04，直接使用scp进行上传。
// 如果使用windows系统，可以安装lrzsz或者使用ftp方式上传至虚拟机
scp jdk-8u212-linux-x64.tar.gz hadoop-3.1.3.tar.gz  v2admin@192.168.122.10:/home/v2admin

scp jdk-8u212-linux-x64.tar.gz hadoop-3.1.3.tar.gz  v2admin@192.168.122.11:/home/v2admin

scp jdk-8u212-linux-x64.tar.gz hadoop-3.1.3.tar.gz  v2admin@192.168.122.12:/home/v2admin

```
### 4.2 安装JDK
```
[v2admin@hadoop10 ~]$tar zxvf jdk-8u212-linux-x64.tar.gz
[v2admin@hadoop10 ~]$sudo mv jdk1.8.0_212/ /usr/local/jdk8
```
### 4.2 安装Hadoop
```
[v2admin@hadoop10 ~]$sudo  tar zxvf hadoop-3.1.3.tar.gz -C /opt
[v2admin@hadoop10 ~]$ sudo chown -R v2admin:v2admin /opt/hadoop-3.1.3  // 修改所属用户和组为当前用户
```
### 4.3 配置jdk和Hadoop环境变量
```
[v2admin@hadoop10 ~]$sudo vim /etc/profile // 最后面添加
......
# set jdk hadoop env
export JAVA_HOME=/usr/local/jdk8
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib

export HADOOP_HOME=/opt/hadoop-3.1.3
export PATH=${PATH}:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
....

[v2admin@hadoop10 ~]$source /etc/profile
[v2admin@hadoop10 ~]$java -version // 验证下jdk
java version "1.8.0_212"
Java(TM) SE Runtime Environment (build 1.8.0_212-b10)
Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)

[v2admin@hadoop10 ~]$hadoop version // 验证下hadoop
Hadoop 3.1.3
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579
Compiled by ztang on 2019-09-12T02:47Z
Compiled with protoc 2.5.0
From source with checksum ec785077c385118ac91aadde5ec9799
This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar

```

### 4.4 分发文件的脚本
因为三台虚拟机配置文件是一样的，如果没有这个脚本，则需要一台一台配置，很繁琐
脚本文件名xrsync.sh 
赋予执行权限,将其放到bin目录下,这样可以像使用其他shell命令一样直接调用
```
#!/bin/bash
if [ $# -lt 1 ]
then
        echo Not Enough Arguement!
fi

for host in hadoop10 hadoop11 hadoop12
do
        echo ============= $host ============
        for file in $@
        do
                if [ -e $file ]
                then
                        pdir=$(cd -P $(dirname $file);pwd)
                        fname=$(basename $file)
                        ssh $host "mkdir -p $pdir"
                        rsync -av $pdir/$fname $host:$pdir
                else
                        echo $file does not exists!
                fi
	done
done

```

### 4.5 集群配置
- 4.5.1 配置下Hadoop的JAVA_HOME
```
[v2admin@hadoop10 ~]$ cd /opt/hadoop-3.1.3/etc/hadoop
[v2admin@hadoop10 ~]$ vim hadoop-env.sh
// 修改JAVA_HOME内容
export JAVA_HOME=/usr/local/jdk8

[v2admin@hadoop10 ~]$xrsync hadoop-env.sh  // 同步更新其他两台主机的配置文件

```

- 4.5.2 核心配置
core-site.xml
```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
   <!-- NameNode的地址 -->
    <property>

        <name>fs.defaultFS</name>

        <value>hdfs://hadoop10:9820</value>

    </property>

 <!-- hadoop 数据的存储目录 -->
    <property>

        <name>hadoop.data.dir</name>

        <value>/opt/module/hadoop-3.1.3/data</value>

    </property>

    <property>

        <name>hadoop.proxyuser.v2admin.hosts</name>

        <value>*</value>

    </property>

    <property>

        <name>hadoop.proxyuser.v2admin.groups</name>

        <value>*</value>

    </property>

<!-- 指定用户 -->
<property>

        <name>hadoop.http.staticuser.user</name>

        <value>v2admin</value>

 </property>
</configuration>
```
- 4.5.3 HDFS 配置
hdfs-site.xml

```
<configuration>
 <!--NameNode数据的存储目录 -->
  <property>

    <name>dfs.namenode.name.dir</name>

    <value>file://${hadoop.data.dir}/name</value>

  </property>

<!--DataNode数据存储目录 -->
  <property>

    <name>dfs.datanode.data.dir</name>

    <value>file://${hadoop.data.dir}/data</value>

  </property>

<!-- 2n数据的存储目录-->
    <property>

    <name>dfs.namenode.checkpoint.dir</name>

    <value>file://${hadoop.data.dir}/namesecondary</value>

  </property>

    <property>

    <name>dfs.client.datanode-restart.timeout</name>

    <value>30</value>

  </property>

<!--nn的WEB访问地址 -->

    <property>

    <name>dfs.namenode.http-address</name>
    <value>hadoop10:9870</value>

  </property>

</configuration>
```

- 4.5.4 Yarn配置
yarn-site.xml
```
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>

        <name>yarn.nodemanager.aux-services</name>

        <value>mapreduce_shuffle</value>

    </property>

    <property>

        <name>yarn.resourcemanager.hostname</name>

        <value>hadoop11</value>

    </property>

    <property>

        <name>yarn.nodemanager.env-whitelist</name>

        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>

    </property>


    <property>

        <name>yarn.log-aggregation-enable</name>

        <value>true</value>

    </property>

    <property>  

        <name>yarn.log.server.url</name>  

        <value>http://hadoop10:19888/jobhistory/logs</value>  

    </property>

    <property>

        <name>yarn.log-aggregation.retain-seconds</name>

        <value>604800</value>

</property>

</configuration>

```

- 4.5.5 MapReduce配置
mapred-site.xml
```
<configuration>

  <property>

    <name>mapreduce.framework.name</name>

    <value>yarn</value>

  </property>

<!-- 历史服务器端地址 -->

<property>

    <name>mapreduce.jobhistory.address</name>

    <value>hadoop10:10020</value>

</property>



<!-- 历史服务器web端地址 -->

<property>

    <name>mapreduce.jobhistory.webapp.address</name>

    <value>hadoop10:19888</value>

</property>

</configuration>

```

- 4.5.6 使用脚本集群上分发配置的好配置文件


### 4.6 启动集群的脚本
启动集群需要在每台服务器上去执行相关启动操作,为方便启动集群,和查看启动信息,编写一个启动脚本startMyCluster.sh
```
#!/bin/bash
if [ $# -lt 1 ]
 then
  echo "Not enough arguments Input !!!"
 exit
fi

case $1 in
# 启动
"start")
	echo "==========start hdfs============="
	ssh hadoop10 /opt/module/hadoop-3.1.3/sbin/start-dfs.sh
	echo "==========start historyServer============"
	ssh hadoop10 /opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver
	echo "==========start yarn============"
	ssh hadoop11 /opt/module/hadoop-3.1.3/sbin/start-yarn.sh

;;
# 关闭
"stop")
        echo "==========stop hdfs============="
        ssh hadoop10 /opt/module/hadoop-3.1.3/sbin/stop-dfs.sh
        echo "==========stop yarn============"
        ssh hadoop11 /opt/module/hadoop-3.1.3/sbin/stop-yarn.sh
	echo "==========stop historyserver===="
	ssh hadoop10 /opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver

;;
# 查看启动信息
"jps")
	for i in hadoop10 hadoop11 hadoop12
	do
		echo "==============$i jps================"
		ssh $i /usr/local/jdk8/bin/jps
	done
;;
*)
 echo "Input Args Error!!!"
;;
esac

```
同样将其放到/bin目录下方便直接调用

### 4.7 启动集群和查看启动信息
```
[v2admin@hadoop10 ~]$ startMyCluster.sh start //启动
==========start hdfs=============
Starting namenodes on [hadoop10]
Starting datanodes
Starting secondary namenodes [hadoop12]
==========start historyServer============
==========start yarn============
Starting resourcemanager
Starting nodemanagers
[v2admin@hadoop10 ~]$ startMyCluster.sh jps //查看启动信息
==============hadoop10 jps================
1831 NameNode
2504 Jps
2265 JobHistoryServer
1980 DataNode
2382 NodeManager
==============hadoop11 jps================
1635 DataNode
1814 ResourceManager
2297 Jps
1949 NodeManager
==============hadoop12 jps================
1795 NodeManager
1590 DataNode
1927 Jps
1706 SecondaryNameNode

```


## 4.8 可能遇到的问题
在安装部署完毕,启动时,可能会遇到**NoClassDefFoundError: javax/activation/DataSource**
我在以前安装时,没有出现过,但之前用的2.x,这次打算安装3.x版本,遇到这个问题,原因是yarn的lib中缺少相关jar包
解决方法:
```
cd /opt/hadoop-3.1.3/share/hadoop/yarn/lib
wget https://repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1.jar
```
